from __future__ import annotations

from typing import Protocol, Any, Tuple, Iterable, Optional


class Backend(Protocol):
    """
    Backend abstraction for array operations.

    All backends must provide consistent API using 'axis' parameter.
    Backends are stateless - no mutable state except device configuration.

    Gradient tracking:
        set_requires_grad is a no-op for non-differentiable backends (NumPy).
        For PyTorch/JAX, it enables gradient tracking for training.
        Only affects tensors explicitly marked for training.
    """
    xp: Any                     # array lib (np, torch, jax, ...)
    float_dtype: Any            # xp.float32
    int_dtype: Any              # xp.int32
    bool_dtype: Any             # xp.bool_

    # creation / conversion
    def asarray(self, x, dtype: Optional[Any] = None): ...
    def zeros(self, shape: Tuple[int, ...], dtype: Optional[Any] = None): ...
    def ones(self, shape: Tuple[int, ...], dtype: Optional[Any] = None): ...
    def zeros_like(self, x): ...
    def ones_like(self, x): ...
    def copy(self, x): ...

    # gradient tracking (no-op for NumPy, enables grad for PyTorch/JAX)
    def set_requires_grad(self, x, requires_grad: bool): ...

    # shape / indexing helpers
    def reshape(self, x, shape: Tuple[int, ...]): ...
    def squeeze(self, x, axis: Optional[int] = None): ...
    def expand_dims(self, x, axis: int): ...
    def concat(self, arrays: Iterable, axis: int = 0): ...
    def stack(self, arrays: Iterable, axis: int = 0): ...
    def broadcast_to(self, x, shape: Tuple[int, ...]): ...

    # math
    def sum(self, x, axis=None, keepdims: bool = False): ...
    def mean(self, x, axis=None, keepdims: bool = False): ...
    def max(self, x, axis=None, keepdims: bool = False): ...
    def min(self, x, axis=None, keepdims: bool = False): ...
    def norm(self, x, ord=None, axis=None, keepdims: bool = False): ...
    def clip(self, x, lo, hi): ...
    def nan_to_num(self, x, nan=0.0, posinf=0.0, neginf=0.0): ...
    def maximum(self, x, y): ...
    def minimum(self, x, y): ...
    def abs(self, x): ...
    def sqrt(self, x): ...

    # comparisons / logic
    def where(self, cond, a, b): ...
    def logical_and(self, a, b): ...
    def logical_or(self, a, b): ...
    def logical_not(self, a): ...
    def isfinite(self, x): ...
    def isinf(self, x): ...
    def isnan(self, x): ...

    # reductions / indexing
    def any(self, x, axis=None, keepdims: bool = False): ...
    def all(self, x, axis=None, keepdims: bool = False): ...
    def argsort(self, x, axis: int = -1): ...
    def argmax(self, x, axis=None): ...
    def argmin(self, x, axis=None): ...
    def nonzero(self, x): ...

    # rng (stateless wrappers taking an integer seed)
    def random(self, seed: int, size): ...
    def choice(self, seed: int, n, p=None): ...
    def randint(self, seed: int, low: int, high: int, size): ...
    def normal(self, seed: int, mean: float, std: float, size): ...

    # host interop
    def to_numpy(self, x): ...

    # JIT compilation
    def jit_compile(self, fn, **kwargs): ...
